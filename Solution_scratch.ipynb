{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as pt\n",
    "import math as ma\n",
    "import scipy as sc\n",
    "## All Checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_ex_1=[np.array([i for i in range(5)]),np.array([i**2 for i in range(3)])]\n",
    "# print(list_ex_1)\n",
    "# list_ex_1[2]=np.array([2,7,8,4])\n",
    "# print(list_ex_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show_img        \n",
    "To load the image\n",
    "# pad_img         \n",
    "To pad the image\n",
    "\n",
    "# Relu            \n",
    "Relu function used\n",
    "# softmax         \n",
    "Softmax activation\n",
    "# activation      \n",
    "The function applying it on the array\n",
    "\n",
    "# c_para_make     \n",
    "Making a list of all nested arrays containing the parameters to learn\n",
    "# f_para_make     \n",
    "Making a list of all nested arrays containing the parameters to learn\n",
    "\n",
    "# nn_make         \n",
    "Making a list of all nested arrays containing the parameters to learn, a list of all the modified data arrays\n",
    "\n",
    "# nn_struc_f      \n",
    "Making a list of all nested arrays containing the known matrices  \n",
    "# nn_struc_c      \n",
    "Making a list of all the nested arrays containing the known matrices \n",
    "\n",
    "# conv            a\n",
    "# max_pol         a\n",
    "# conv_tr_layer   a\n",
    "# nn_fwd_layer    a\n",
    "\n",
    "# forward_prop    a\n",
    "# ack_prop        a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dataset in the required form\n",
    "global row_pixel, col_pixel\n",
    "\n",
    "df_base=pd.read_csv('train.csv')\n",
    "df=np.array(df_base.iloc[:,1:])\n",
    "target=np.array(df_base.iloc[:,0])\n",
    "df_sh=df.shape\n",
    "row_pixel=col_pixel=int(ma.sqrt(df_sh[1]))\n",
    "df=df.reshape(df_sh[0],row_pixel,col_pixel)\n",
    "df_sh=df.shape\n",
    "m=df.shape[0]\n",
    "#  m is int not ndarray int type\n",
    "## All Checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=np.array([i for i in range(10)])\n",
    "# to print an image\n",
    "def show_img(array):\n",
    "    pt.imshow(array,cmap='gray')\n",
    "    pt.show()\n",
    "def pad_img(array,p):\n",
    "    ar=array.copy()\n",
    "    n=row_pixel\n",
    "    h_pad_0=np.zeros((n,1), dtype=int)\n",
    "    y=np.hstack([h_pad_0]*p)\n",
    "    x=np.hstack((y,array,y))\n",
    "    v_pad_1=np.zeros((1,n+2*p), dtype=int)\n",
    "    y=np.vstack([v_pad_1]*p)\n",
    "    x=np.vstack((y,x,y))\n",
    "    return x\n",
    "# a_1=pad_img(df[1],2)\n",
    "# print(a_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, (3, 3), 1, (2, 2), 2], [1, (3, 3), 1, (2, 2), 1]]\n"
     ]
    }
   ],
   "source": [
    "# Example of nn_structure variable\n",
    "nn_struc_c=[[3,(3,3),1,(2,2),2,'Relu'],[1,(3,3),1,(2,2),1,'sigmoid']]\n",
    "# ---------_^\n",
    "# Layers marked\n",
    "# format of variables (below)\n",
    "# 'ReLu','sigmoid','tanh','softmax'\n",
    "# [no. of layers->[filters,(filter_row,filter_row),padding,(pol_filter_row,pol_filter_row),pol_stride,func]...]\n",
    "print(nn_struc_c)\n",
    "\n",
    "\"\"\"\n",
    "A single layer's information as we want to get the idea of convolution layers\n",
    "For feature layers we define a separate structure  \n",
    "\"\"\"\n",
    "\n",
    "nn_struc_f=[100,40,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytesize(lis):\n",
    "    l=len(lis)\n",
    "    c=0\n",
    "    for i in range(l):\n",
    "        c+=lis[i].size*lis[i].itemsize\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate the list of array of parameters for all convolution layers\n",
    "def c_para_constr(rows,nn_struc_c):\n",
    "    row_iter=rows\n",
    "    c_layers=len(nn_struc_c)\n",
    "    c_wt_list=[]\n",
    "    c_b_list=[]\n",
    "    c_pol_list=[]\n",
    "    for i in range(c_layers):\n",
    "        filters=nn_struc_c[i][0]\n",
    "        conv_filter_row=nn_struc_c[i][1][0]\n",
    "        padding=nn_struc_c[i][2]\n",
    "        pol_stride=nn_struc_c[i][4]\n",
    "        new_row=ma.floor((row_iter-conv_filter_row+2*padding)/pol_stride)+1\n",
    "        c_wt_list.append(np.random.rand(filters,new_row,new_row))\n",
    "        c_b_list.append(np.random.rand(filters,new_row))\n",
    "        # pol_element=np.array([nn_struc_c[i][3],nn_struc_c[i][4]])\n",
    "        # c_pol_list.append(pol_element)\n",
    "        row_iter=new_row\n",
    "    total_features=c_b_list[-1].size\n",
    "    return c_wt_list,c_b_list,total_features#,c_pol_list\n",
    "\n",
    "# To generate the list of array of parameters for all feature layers \n",
    "def f_para_constr(nn_struc_f,init_features):\n",
    "    f_wt_list=[]\n",
    "    f_b_list=[]\n",
    "    features=init_features\n",
    "    f_layers=len(nn_struc_f)\n",
    "    for i in range(f_layers):\n",
    "        f_wt_list.append(np.random.rand(nn_struc_f[i],features))\n",
    "        f_b_list.append(np.random.rand(features,1))\n",
    "        features=nn_struc_f[i]\n",
    "    return f_wt_list,f_b_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_make(nn_struc_c,nn_struc_f,c_b,f_b):\n",
    "    list_c=[]\n",
    "    ar_f=np.array(nn_struc_f)\n",
    "    list_all=[]\n",
    "    layer_c=len(c_b)\n",
    "    layer_f=len(f_b)\n",
    "    n=row_pixel\n",
    "    for i in range (layer_c):\n",
    "        element=[]\n",
    "        ax_0=nn_struc_c[i][0]\n",
    "        c_row=nn_struc_c[i][1][0]\n",
    "        padding=nn_struc_c[i][2]\n",
    "        pol_row=nn_struc_c[i][3][0]\n",
    "        pol_stride=nn_struc_c[i][4]\n",
    "        ax_1=ma.floor((n-c_row+2*padding)/pol_stride)+1\n",
    "        new_dim_c=(ax_0,ax_1,ax_1)\n",
    "        ax_1=ma.floor((ax_1-pol_row)/pol_stride)+1\n",
    "        new_dim_p=(ax_0,ax_1,ax_1)\n",
    "        element.append(new_dim_c)\n",
    "        element.append(new_dim_p)\n",
    "        list_c.append(element)\n",
    "        n=ax_1\n",
    "    init_features=element[1].size\n",
    "    list_all.append(list_c)\n",
    "    list_all.append(init_features)\n",
    "    list_all.append(ar_f)\n",
    "    return list_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_001,a_002,a_003=c_para_constr(row_pixel,nn_struc_c)\n",
    "# print(a_001,a_002,a_003)\n",
    "# print(bytesize(a_001)+bytesize(a_002))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectified Linear Unit\n",
    "def ReLu(array):\n",
    "    array_0=np.zeros(array.shape,dtype=float)    \n",
    "    array=np.maximum(array,array_0)\n",
    "    return array\n",
    "\n",
    "def softmax(array):\n",
    "    array=np.exp(array)\n",
    "    array=array/(np.sum(array))\n",
    "    return array\n",
    "\n",
    "# For the activation\n",
    "def activation(array,func):\n",
    "    if func=='ReLu':\n",
    "        array=ReLu(array)\n",
    "    elif func=='sigmoid':\n",
    "        array=1/(1+np.exp((-1)*array))\n",
    "    elif func=='tanh':\n",
    "        array=np.tanh(array)\n",
    "    elif func=='softmax':\n",
    "        array=softmax(array)\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For the cost function\n",
    "def cost_array(expected,label):\n",
    "    cost=(label-expected)**2\n",
    "    return cost_array\n",
    "\n",
    "# For the derivatives\n",
    "def p_diff_cust(variable,func):\n",
    "    if func=='cost':\n",
    "        derivative=2*np.sqrt(variable)\n",
    "    elif func=='ReLu':\n",
    "        derivative=variable>0\n",
    "        derivative+=0\n",
    "    elif func=='sigmoid':\n",
    "        derivative=variable-variable**2\n",
    "    elif func=='tanh':\n",
    "        derivative=1-variable**2\n",
    "    elif func=='softmax':\n",
    "        derivative=variable-variable**2\n",
    "    return derivative\n",
    "\n",
    "def learn_part():\n",
    "\n",
    "    return\n",
    "\n",
    "# For the calculation of gradient for an entire layer \n",
    "def calc_grad_c_layer():\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution Layer\n",
    "def conv(dat,layer_info,c_wt_layer,c_b_layer):\n",
    "    no_filters=layer_info[0]\n",
    "    r=layer_info[1]\n",
    "    padding=layer_info[2]\n",
    "    n=row_pixel\n",
    "    dat_pad=pad_img(dat,layer_info[2])\n",
    "    nn_sample=np.vstack([dat]*no_filters)\n",
    "    temp_conv_row=n-r+1\n",
    "    ar=np.zeros((no_filters,temp_conv_row,temp_conv_row),dtype=float)\n",
    "    for i in range(r):\n",
    "        for j in range(r):\n",
    "            t=(n-r+i+1)\n",
    "            u=(n-r+j+1)\n",
    "            # bradcast might be needed of c_wt_layer\n",
    "            ar[:,i:t:r,j:t:r]=c_wt_layer[:,i,j]*nn_sample[:,i:t:r,j:t:r]\n",
    "    c_b_array=np.array([np.full_like(ar,c_b_layer[i],dtype=float)])\n",
    "    ar=ar+c_b_layer\n",
    "    return ar\n",
    "\n",
    "# Pooling layer\n",
    "def max_pol(ar,layer_info):\n",
    "    n=row_pixel\n",
    "    pol_row=layer_info[3][0]\n",
    "    stride=layer_info[4]\n",
    "    temp_row=ma.floor((n-pol_row)/stride)+1\n",
    "    pol=np.zeros(np.shape(ar[:,:temp_row:stride,:temp_row:stride]),dtype=float)\n",
    "    for i in range (pol_row):\n",
    "        for j in range (pol_row):\n",
    "            pol=np.maximum(ar[:,i:(temp_row+i):stride,j:(temp_row+j):stride],pol)\n",
    "    return pol\n",
    "\n",
    "# Transforming laer consisting both conv() and max_pol() \n",
    "def conv_tr_layer(dat,layer_info,func,c_wt_layer,c_b_layer,list_all_layer):\n",
    "    ar_01=dat.copy()\n",
    "    ar_01=conv(ar,layer_info,c_wt_layer,c_b_layer)\n",
    "    list_all_layer[0]=ar\n",
    "    ar=max_pol(ar,layer_info)\n",
    "    ar=activation(ar,func)\n",
    "    list_all_layer[1]=ar\n",
    "    return ar\n",
    "\n",
    "# For forward neural network propagation over a single layer\n",
    "def nn_fwd_layer(prev,num_new,func,f_wt_layer,f_b_layer):\n",
    "    new=np.add((f_wt_layer@prev),f_b_layer)\n",
    "    new=activation(new,func)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For forward propagation over decided layers\n",
    "def forward_prop(dat,c_wt_list,c_b_list,nn_struc_c,nn_struc_f,list_all):\n",
    "    l_c=len(nn_struc_c)\n",
    "    ar=dat.copy()\n",
    "    for i in range(l_c):\n",
    "        new_ar=conv_tr_layer(ar,c_wt_list[i],c_b_list[i],list_all[i])\n",
    "        ar=new_ar\n",
    "    init_fc_nodes=list_all[1]\n",
    "    l_f=len(nn_struc_f)\n",
    "    for i in range(l_f):\n",
    "        final_fc_nodes=nn_struc_f[i]\n",
    "        new_ar=nn_fwd_layer(init_fc_nodes,final_fc_nodes,list_all[2][i][0],list_all[2][i][1])\n",
    "    return ar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For backward neural network propagation\n",
    "def back_prop():\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### multi dimensional matrix multiplication works when used with \n",
    "# x=np.array([i for i in range(9)]).reshape(3,3)\n",
    "# print(x)\n",
    "# y=np.vstack([x]*3)\n",
    "# print(y)\n",
    "# y=y.reshape(3,3,3)\n",
    "# print(y)\n",
    "# z=np.array([12,13,14]).reshape(3,1)\n",
    "# print(z)\n",
    "# print(y+z)\n",
    "# ansxz=x@z\n",
    "# print(ansxz)\n",
    "# z=np.vstack([z]*3).reshape(3,3,1)\n",
    "# print(z)\n",
    "# print(y@z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n",
      "[[0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# x_exp=np.array([i for i in range(9)]).reshape(3,3)\n",
    "# y_exp=x_exp>0\n",
    "# print(y_exp)\n",
    "# z_exp=y_exp+0\n",
    "# print(z_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
